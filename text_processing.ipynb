{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69acc594-058a-4c60-b008-36914ade9d9d",
   "metadata": {},
   "source": [
    "<b>Notebook Introduction:</b> This notebook contains functions used for text processing of the extracted subreddit data, e.g., tokenization/bigrams, lemmatization, removal of hyperlinks and other noise. The cleaned text is used for topic modeling in topic_modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb8a3cf-42ba-484a-8bef-e45350e34bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#for text processing\n",
    "from string import punctuation\n",
    "punc_list = list(punctuation)\n",
    "punc_list.remove('/') #needed for references to subreddits\n",
    "\n",
    "import regex as re\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #working with gensim for this project to gain exp w/ this library\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87cc22db-7315-49e2-9168-8984a8aa6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_df(path, city):\n",
    "    \"\"\"for each subreddit (variable = city), this function consolidates the text from the posts, comments and comment replies extracted,\n",
    "    to create a dataframe of close to 50K records for further analysis\"\"\"\n",
    "    \n",
    "    os.chdir(path)\n",
    "    cons_df = pd.DataFrame()\n",
    "\n",
    "    # looping through and reading each file in that directory that references this city\n",
    "    # these would be the (1)posts df, (2)comments df and (3)replies df\n",
    "    for file in os.listdir():\n",
    "        if city in file:\n",
    "            _ = pd.read_pickle(file)\n",
    "            cons_df = pd.concat([cons_df, _.text])\n",
    "            cons_df.reset_index(drop = True, inplace = True)\n",
    "    cons_df.columns = [\"text\"]\n",
    "    cons_df = cons_df[cons_df[\"text\"] != \"[deleted]\"] #excluding deleted posts\n",
    "    \n",
    "    return cons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f230da-5677-43d8-85f7-cb3d166db282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(x):\n",
    "    \"\"\"returns the tokenized and lemmatized words from the text provided\"\"\"\n",
    "\n",
    "    #using regex to remove (1) gifs or (2) hyperlinks\n",
    "    x = re.sub(r\"!\\[gif\\]\\(emote\\|free_emotes_pack\\|\\w+\\)|!\\[gif\\]\\(giphy\\|\\w+\\)\", \"\", x)  \n",
    "    x = re.sub(r\"http.+\\b\", \"\", x) #hyperlinks\n",
    "    # x = re.sub(r\"[^A-Za-z\\ +]\", \"\", x) #non-alphabetic character based words\n",
    "\n",
    "    #tokenization and lemmatization\n",
    "    doc = nlp(x)\n",
    "    token_str =  \" \".join([str(word.lemma_).lower().strip() for word in doc if str(word).lower() not in STOP_WORDS and str(word) not in punc_list]) # and len(str(word).strip())>2]\n",
    "    \n",
    "    #the tokens need to be tweaked a bit to to include mentions of subreddits\n",
    "    token_list = re.sub(\"r / \", \"r/\", token_str).split(\" \") \n",
    "    #this should be tweaked to y = re.sub(\"^r / \", otherwises things like 'sambhajinagar / aurangabad' are also captured but I dont wanna rerun this \n",
    "    #(takes too long) and there is no material impact\n",
    "\n",
    "    #dropping anything that is punctuations only or short words\n",
    "    token_list = [token for token in token_list if len(token)>3 and bool(re.fullmatch(r'^[\\W_]+$', token)) == False]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f25c098e-446f-4b51-84eb-b1f7b45b3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## appending bigrams\n",
    "def create_bigrams(df):\n",
    "    \"\"\"function to append common bigrams to the tokens. something similar can be done with sklearn CountVectorizer too but I want to get\n",
    "    some experience with gensim\"\"\"\n",
    "    \n",
    "    bigram = Phrases(df[\"processed_text\"], min_count=10)\n",
    "    bigram_phraser = Phraser(bigram)\n",
    "    df[\"bigrams\"] = df[\"processed_text\"].apply(lambda x: list(bigram_phraser[x]))\n",
    "    df[\"processed_text_bigrams\"] = df[[\"processed_text\", \"bigrams\"]].apply(lambda x: x[0] + [word for word in x[1] if word not in x[0]], \n",
    "                                                         axis = 1)\n",
    "    # df.drop(columns = \"bigrams\", inplace = True) #for record keeping, I'm not dropping the original processed_text or the bigram columns\n",
    "    # the dataframe is not too large so this should not be a problem\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec75175e-9f8a-46fd-aec5-4f946dfaae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_func_call(city):\n",
    "    start = time.time() \n",
    "\n",
    "    #creating the base dataframe\n",
    "    df = create_base_df('/home/prabhur/reddit_project/data/raw/', city)\n",
    "    print(f\"{df.shape[0]} records from this subreddit were analyzed\")\n",
    "    \n",
    "    os.chdir('/home/prabhur/reddit_project/data/')\n",
    "    #text processing\n",
    "    df[\"processed_text\"] = df[\"text\"].apply(lambda x: text_processing(x))\n",
    "    \n",
    "    #creating the bigrams\n",
    "    df = create_bigrams(df)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Text processing takes ~{round((end-start)/60,2)} mins\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "515041b3-fc15-476f-af00-7435e0ff7b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49940 records from this subreddit were analyzed\n",
      "Text processing takes ~5.61 mins\n"
     ]
    }
   ],
   "source": [
    "##version 1:\n",
    "print(\"Delhi\")\n",
    "del_df = combined_func_call('del')\n",
    "del_df.to_pickle(\"del_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "886a3119-6de6-466e-b340-000cda71482f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48551 records from this subreddit were analyzed\n",
      "Text processing takes ~5.89 mins\n"
     ]
    }
   ],
   "source": [
    "mum_df = combined_func_call('mum')\n",
    "mum_df.to_pickle(\"mum_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5945c58c-d87e-459b-a9b5-6dbbce88fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47843 records from this subreddit were analyzed\n",
      "Text processing takes ~6.76 mins\n"
     ]
    }
   ],
   "source": [
    "ban_df = combined_func_call('ban')\n",
    "ban_df.to_pickle(\"ban_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06db5d9c-40a0-4087-a2e2-a1c3fb2c29de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56433 records from this subreddit were analyzed\n",
      "Text processing takes ~7.32 mins\n"
     ]
    }
   ],
   "source": [
    "##version 2\n",
    "nyc_df = combined_func_call('nyc')\n",
    "nyc_df.to_pickle(\"nyc_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c811088-6fa4-430c-b9f1-2b1da8e0064e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51503 records from this subreddit were analyzed\n",
      "Text processing takes ~6.64 mins\n",
      "51350 records from this subreddit were analyzed\n",
      "Text processing takes ~6.91 mins\n"
     ]
    }
   ],
   "source": [
    "chi_df = combined_func_call('chi')\n",
    "chi_df.to_pickle(\"chi_df.pkl\")\n",
    "\n",
    "bos_df = combined_func_call('bos')\n",
    "bos_df.to_pickle(\"bos_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a97a871-c4c2-4975-af92-bf8585f56590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5453ec-99db-41a3-8e11-90175ad5a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langdetect  #to check for english language\n",
    "# ##https://pypi.org/project/langdetect/\n",
    "\n",
    "# from langdetect import detect\n",
    "# test = del_df.iloc[:3,:]\n",
    "# # test\n",
    "# y = detect(test.loc[0,\"text\"])\n",
    "# y\n",
    "\n",
    "##tried language detect, both on the original text and also after tokenization and cleanup. But langdetect did not have good results\n",
    "##upon further research it seems like this may be because langdetect needs longer sentences to detect the language correctly.\n",
    "# test[\"lang\"] = test[\"text\"].apply(lambda x: detect(x)) \n",
    "# test\n",
    "# del_df[\"lang\"].nunique()\n",
    "# del_df)\n",
    "\n",
    "# from string import punctuation\n",
    "# # import nltk #too slow with tokenization\n",
    "# # test[\"text2\"] = test[\"text\"].apply(lambda x: \" \".join([word.strip().lower() for word in x.split(\" \") if word not in punctuation])) #langdetect alone did not have good results\n",
    "# test[\"lang2\"] = test[\"text2\"].apply(lambda x: detect(x)) #langdetect alone did not have good results\n",
    "# test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
